{
  "instance_id": "psf__requests-1142",
  "run_id": "phase_a_v0_verified_280_300_20260129_171549",
  "subset": "verified",
  "split": "test",
  "config": "/app/src/minisweagent/config/extra/swebench_modal_litellm.yaml",
  "start_time": "2026-01-29T09:49:06.886247",
  "status": "success",
  "returncode": 0,
  "stdout": "rned capitalised\n+            k = k.lower()\n+\n+            has_value = headers.get(k)\n+            if has_value: # Python 3: Repeating header keys are unmerged.\n+                v = ', '.join([has_value, v])\n+\n+            headers[k] = v\n+\n+        # HTTPResponse objects in Python 3 don't have a .strict attribute\n+        strict = getattr(r, 'strict', 0)\n+        return ResponseCls(body=r,\n+                           headers=headers,\n+                           status=r.status,\n+                           version=r.version,\n+                           reason=r.reason,\n+                           strict=strict,\n+                           original_response=r,\n+                           **response_kw)\n+\n+    # Backwards-compatibility methods for httplib.HTTPResponse\n+    def getheaders(self):\n+        return self.headers\n+\n+    def getheader(self, name, default=None):\n+        return self.headers.get(name, default)\ndiff --git a/build/lib/requests/packages/urllib3/util.py \nb/build/lib/requests/packages/urllib3/util.py\nnew file mode 100644\nindex 0000000..b827bc4\n--- /dev/null\n+++ b/build/lib/requests/packages/urllib3/util.py\n@@ -0,0 +1,338 @@\n+# urllib3/util.py\n+# Copyright 2008-2012 Andrey Petrov and contributors (see CONTRIBUTORS.txt)\n+#\n+# This module is part of urllib3 and is released under\n+# the MIT License: http://www.opensource.org/licenses/mit-license.php\n+\n+\n+from base64 import b64encode\n+from collections import namedtuple\n+from socket import error as SocketError\n+\n+try:\n+    from select import poll, POLLIN\n+except ImportError:  # `poll` doesn't exist on OSX and other platforms\n+    poll = False\n+    try:\n+        from select import select\n+    except ImportError:  # `select` doesn't exist on AppEngine.\n+        select = False\n+\n+try:  # Test for SSL features\n+    SSLContext = None\n+    HAS_SNI = False\n+\n+    import ssl\n+    from ssl import wrap_socket, CERT_NONE, SSLError, PROTOCOL_SSLv23\n+    from ssl import SSLContext  # Modern SSL?\n+    from ssl import HAS_SNI  # Has SNI?\n+except ImportError:\n+    pass\n+\n+\n+from .packages import six\n+from .exceptions import LocationParseError\n+\n+\n+class Url(namedtuple('Url', ['scheme', 'auth', 'host', 'port', 'path', 'query',\n'fragment'])):\n+    \"\"\"\n+    Datastructure for representing an HTTP URL. Used as a return value for\n+    :func:`parse_url`.\n+    \"\"\"\n+    slots = ()\n+\n+    def __new__(cls, scheme=None, auth=None, host=None, port=None, path=None, \nquery=None, fragment=None):\n+        return super(Url, cls).__new__(cls, scheme, auth, host, port, path, \nquery, fragment)\n+\n+    @property\n+    def hostname(self):\n+        \"\"\"For backwards-compatibility with urlparse. We're nice like that.\"\"\"\n+        return self.host\n+\n+    @property\n+    def request_uri(self):\n+        \"\"\"Absolute path including the query string.\"\"\"\n+        uri = self.path or '/'\n+\n+        if self.query is not None:\n+            uri += '?' + self.query\n+\n+        return uri\n+\n+\n+def split_first(s, delims):\n+    \"\"\"\n+    Given a string and an iterable of delimiters, split on the first found\n+    delimiter. Return two split parts and the matched delimiter.\n+\n+    If not found, then the first part is the full input string.\n+\n+    Example: ::\n+\n+        >>> split_first('foo/bar?baz', '?/=')\n+        ('foo', 'bar?baz', '/')\n+        >>> split_first('foo/bar?baz', '123')\n+        ('foo/bar?baz', '', None)\n+\n+    Scales linearly with number of delims. Not ideal for large number of \ndelims.\n+    \"\"\"\n+    min_idx = None\n+    min_delim = None\n+    for d in delims:\n+        idx = s.find(d)\n+        if idx < 0:\n+            continue\n+\n+        if min_idx is None or idx < min_idx:\n+            min_idx = idx\n+            min_delim = d\n+\n+    if min_idx is None or min_idx < 0:\n+        return s, '', None\n+\n+    return s[:min_idx], s[min_idx+1:], min_delim\n+\n+\n+def parse_url(url):\n+    \"\"\"\n+    Given a url, return a parsed :class:`.Url` namedtuple. Best-effort is\n+    performed to parse incomplete urls. Fields not provided will be None.\n+\n+    Partly backwards-compatible with :mod:`urlparse`.\n+\n+    Example: ::\n+\n+        >>> parse_url('http://google.com/mail/')\n+        Url(scheme='http', host='google.com', port=None, path='/', ...)\n+        >>> parse_url('google.com:80')\n+        Url(scheme=None, host='google.com', port=80, path=None, ...)\n+        >>> parse_url('/foo?bar')\n+        Url(scheme=None, host=None, port=None, path='/foo', query='bar', ...)\n+    \"\"\"\n+\n+    # While this code has overlap with stdlib's urlparse, it is much\n+    # simplified for our needs and less annoying.\n+    # Additionally, this imeplementations does silly things to be optimal\n+    # on CPython.\n+\n+    scheme = None\n+    auth = None\n+    host = None\n+    port = None\n+    path = None\n+    fragment = None\n+    query = None\n+\n+    # Scheme\n+    if '://' in url:\n+        scheme, url = url.split('://', 1)\n+\n+    # Find the earliest Authority Terminator\n+    # (http://tools.ietf.org/html/rfc3986#section-3.2)\n+    url, path_, delim = split_first(url, ['/', '?', '#'])\n+\n+    if delim:\n+        # Reassemble the path\n+        path = delim + path_\n+\n+    # Auth\n+    if '@' in url:\n+        auth, url = url.split('@', 1)\n+\n+    # IPv6\n+    if url and url[0] == '[':\n+        host, url = url[1:].split(']', 1)\n+\n+    # Port\n+    if ':' in url:\n+        _host, port = url.split(':', 1)\n+\n+        if not host:\n+            host = _host\n+\n+        if not port.isdigit():\n+            raise LocationParseError(\"Failed to parse: %s\" % url)\n+\n+        port = int(port)\n+\n+    elif not host and url:\n+        host = url\n+\n+    if not path:\n+        return Url(scheme, auth, host, port, path, query, fragment)\n+\n+    # Fragment\n+    if '#' in path:\n+        path, fragment = path.split('#', 1)\n+\n+    # Query\n+    if '?' in path:\n+        path, query = path.split('?', 1)\n+\n+    return Url(scheme, auth, host, port, path, query, fragment)\n+\n+\n+def get_host(url):\n+    \"\"\"\n+    Deprecated. Use :func:`.parse_url` instead.\n+    \"\"\"\n+    p = parse_url(url)\n+    return p.scheme or 'http', p.hostname, p.port\n+\n+\n+def make_headers(keep_alive=None, accept_encoding=None, user_agent=None,\n+                 basic_auth=None):\n+    \"\"\"\n+    Shortcuts for generating request headers.\n+\n+    :param keep_alive:\n+        If ``True``, adds 'connection: keep-alive' header.\n+\n+    :param accept_encoding:\n+        Can be a boolean, list, or string.\n+        ``True`` translates to 'gzip,deflate'.\n+        List will get joined by comma.\n+        String will be used as provided.\n+\n+    :param user_agent:\n+        String representing the user-agent you want, such as\n+        \"python-urllib3/0.6\"\n+\n+    :param basic_auth:\n+        Colon-separated username:password string for 'authorization: basic ...'\n+        auth header.\n+\n+    Example: ::\n+\n+        >>> make_headers(keep_alive=True, user_agent=\"Batman/1.0\")\n+        {'connection': 'keep-alive', 'user-agent': 'Batman/1.0'}\n+        >>> make_headers(accept_encoding=True)\n+        {'accept-encoding': 'gzip,deflate'}\n+    \"\"\"\n+    headers = {}\n+    if accept_encoding:\n+        if isinstance(accept_encoding, str):\n+            pass\n+        elif isinstance(accept_encoding, list):\n+            accept_encoding = ','.join(accept_encoding)\n+        else:\n+            accept_encoding = 'gzip,deflate'\n+        headers['accept-encoding'] = accept_encoding\n+\n+    if user_agent:\n+        headers['user-agent'] = user_agent\n+\n+    if keep_alive:\n+        headers['connection'] = 'keep-alive'\n+\n+    if basic_auth:\n+        headers['authorization'] = 'Basic ' + \\\n+            b64encode(six.b(basic_auth)).decode('utf-8')\n+\n+    return headers\n+\n+\n+def is_connection_dropped(conn):\n+    \"\"\"\n+    Returns True if the connection is dropped and should be closed.\n+\n+    :param conn:\n+        :class:`httplib.HTTPConnection` object.\n+\n+    Note: For platforms like AppEngine, this will always return ``False`` to\n+    let the platform handle connection recycling transparently for us.\n+    \"\"\"\n+    sock = getattr(conn, 'sock', False)\n+    if not sock: # Platform-specific: AppEngine\n+        return False\n+\n+    if not poll: # Platform-specific\n+        if not select: # Platform-specific: AppEngine\n+            return False\n+\n+        try:\n+            return select([sock], [], [], 0.0)[0]\n+        except SocketError:\n+            return True\n+\n+    # This version is better on platforms that support it.\n+    p = poll()\n+    p.register(sock, POLLIN)\n+    for (fno, ev) in p.poll(0.0):\n+        if fno == sock.fileno():\n+            # Either data is buffered (bad), or the connection is dropped.\n+            return True\n+\n+\n+def resolve_cert_reqs(candidate):\n+    \"\"\"\n+    Resolves the argument to a numeric constant, which can be passed to\n+    the wrap_socket function/method from the ssl module.\n+    Defaults to :data:`ssl.CERT_NONE`.\n+    If given a string it is assumed to be the name of the constant in the\n+    :mod:`ssl` module or its abbrevation.\n+    (So you can specify `REQUIRED` instead of `CERT_REQUIRED`.\n+    If it's neither `None` nor a string we assume it is already the numeric\n+    constant which can directly be passed to wrap_socket.\n+    \"\"\"\n+    if candidate is None:\n+        return CERT_NONE\n+\n+    if isinstance(candidate, str):\n+        res = getattr(ssl, candidate, None)\n+        if res is None:\n+            res = getattr(ssl, 'CERT_' + candidate)\n+        return res\n+\n+    return candidate\n+\n+\n+def resolve_ssl_version(candidate):\n+    \"\"\"\n+    like resolve_cert_reqs\n+    \"\"\"\n+    if candidate is None:\n+        return PROTOCOL_SSLv23\n+\n+    if isinstance(candidate, str):\n+        res = getattr(ssl, candidate, None)\n+        if res is None:\n+            res = getattr(ssl, 'PROTOCOL_' + candidate)\n+        return res\n+\n+    return candidate\n+\n+if SSLContext is not None:  # Python 3.2+\n+    def ssl_wrap_socket(sock, keyfile=None, certfile=None, cert_reqs=None,\n+                        ca_certs=None, server_hostname=None,\n+                        ssl_version=None):\n+        \"\"\"\n+        All arguments except `server_hostname` have the same meaning as for\n+        :func:`ssl.wrap_socket`\n+\n+        :param server_hostname:\n+            Hostname of the expected certificate\n+        \"\"\"\n+        context = SSLContext(ssl_version)\n+        context.verify_mode = cert_reqs\n+        if ca_certs:\n+            try:\n+                context.load_verify_locations(ca_certs)\n+            # Py32 raises IOError\n+            # Py33 raises FileNotFoundError\n+            except Exception as e:  # Reraise as SSLError\n+                raise SSLError(e)\n+        if certfile:\n+            # FIXME: This block needs a test.\n+            context.load_cert_chain(certfile, keyfile)\n+        if HAS_SNI:  # Platform-specific: OpenSSL with enabled SNI\n+            return context.wrap_socket(sock, server_hostname=server_hostname)\n+        return context.wrap_socket(sock)\n+\n+else:  # Python 3.1 and earlier\n+    def ssl_wrap_socket(sock, keyfile=None, certfile=None, cert_reqs=None,\n+                        ca_certs=None, server_hostname=None,\n+                        ssl_version=None):\n+        return wrap_socket(sock, keyfile=keyfile, certfile=certfile,\n+                           ca_certs=ca_certs, cert_reqs=cert_reqs,\n+                           ssl_version=ssl_version)\ndiff --git a/build/lib/requests/sessions.py b/build/lib/requests/sessions.py\nnew file mode 100644\nindex 0000000..2687cb6\n--- /dev/null\n+++ b/build/lib/requests/sessions.py\n@@ -0,0 +1,408 @@\n+# -*- coding: utf-8 -*-\n+\n+\"\"\"\n+requests.session\n+~~~~~~~~~~~~~~~~\n+\n+This module provides a Session object to manage and persist settings across\n+requests (cookies, auth, proxies).\n+\n+\"\"\"\n+import os\n+\n+from .compat import cookielib\n+from .cookies import cookiejar_from_dict\n+from .models import Request\n+from .hooks import default_hooks, dispatch_hook\n+from .utils import from_key_val_list, default_headers\n+from .exceptions import TooManyRedirects, InvalidSchema\n+\n+from .compat import urlparse, urljoin\n+from .adapters import HTTPAdapter\n+\n+from .utils import requote_uri, get_environ_proxies, get_netrc_auth\n+\n+from .status_codes import codes\n+REDIRECT_STATI = (codes.moved, codes.found, codes.other, codes.temporary_moved)\n+DEFAULT_REDIRECT_LIMIT = 30\n+\n+\n+def merge_kwargs(local_kwarg, default_kwarg):\n+    \"\"\"Merges kwarg dictionaries.\n+\n+    If a local key in the dictionary is set to None, it will be removed.\n+    \"\"\"\n+\n+    if default_kwarg is None:\n+        return local_kwarg\n+\n+    if isinstance(local_kwarg, str):\n+        return local_kwarg\n+\n+    if local_kwarg is None:\n+        return default_kwarg\n+\n+    # Bypass if not a dictionary (e.g. timeout)\n+    if not hasattr(default_kwarg, 'items'):\n+        return local_kwarg\n+\n+    default_kwarg = from_key_val_list(default_kwarg)\n+    local_kwarg = from_key_val_list(local_kwarg)\n+\n+    # Update new values in a case-insensitive way\n+    def get_original_key(original_keys, new_key):\n+        \"\"\"\n+        Finds the key from original_keys that case-insensitive matches new_key.\n+        \"\"\"\n+        for original_key in original_keys:\n+            if key.lower() == original_key.lower():\n+                return original_key\n+        return new_key\n+\n+    kwargs = default_kwarg.copy()\n+    original_keys = kwargs.keys()\n+    for key, value in local_kwarg.items():\n+        kwargs[get_original_key(original_keys, key)] = value\n+\n+    # Remove keys that are set to None.\n+    for (k, v) in local_kwarg.items():\n+        if v is None:\n+            del kwargs[k]\n+\n+    return kwargs\n+\n+\n+class SessionRedirectMixin(object):\n+\n+    def resolve_redirects(self, resp, req, stream=False, timeout=None, \nverify=True, cert=None, proxies=None):\n+        \"\"\"Receives a Response. Returns a generator of Responses.\"\"\"\n+\n+        i = 0\n+\n+        # ((resp.status_code is codes.see_other))\n+        while (('location' in resp.headers and resp.status_code in \nREDIRECT_STATI)):\n+\n+            resp.content  # Consume socket so it can be released\n+\n+            if i >= self.max_redirects:\n+                raise TooManyRedirects('Exceeded %s redirects.' % \nself.max_redirects)\n+\n+            # Release the connection back into the pool.\n+            resp.close()\n+\n+            url = resp.headers['location']\n+            method = req.method\n+\n+            # Handle redirection without scheme (see: RFC 1808 Section 4)\n+            if url.startswith('//'):\n+                parsed_rurl = urlparse(resp.url)\n+                url = '%s:%s' % (parsed_rurl.scheme, url)\n+\n+            # Facilitate non-RFC2616-compliant 'location' headers\n+            # (e.g. '/path/to/resource' instead of \n'http://domain.tld/path/to/resource')\n+            if not urlparse(url).netloc:\n+                # Compliant with RFC3986, we percent encode the url.\n+                url = urljoin(resp.url, requote_uri(url))\n+\n+            # http://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html#sec10.3.4\n+            if resp.status_code is codes.see_other and req.method != 'HEAD':\n+                method = 'GET'\n+\n+            # Do what the browsers do, despite standards...\n+            if resp.status_code in (codes.moved, codes.found) and req.method ==\n'POST':\n+                method = 'GET'\n+\n+            # Remove the cookie headers that were sent.\n+            headers = req.headers\n+            try:\n+                del headers['Cookie']\n+            except KeyError:\n+                pass\n+\n+            resp = self.request(\n+                    url=url,\n+                    method=method,\n+                    headers=headers,\n+                    auth=req.auth,\n+                    cookies=req.cookies,\n+                    allow_redirects=False,\n+                    stream=stream,\n+                    timeout=timeout,\n+                    verify=verify,\n+                    cert=cert,\n+                    proxies=proxies,\n+                    hooks=req.hooks,\n+            )\n+\n+            i += 1\n+            yield resp\n+\n+\n+class Session(SessionRedirectMixin):\n+    \"\"\"A Requests session.\n+\n+    Provides cookie persistience, connection-pooling, and configuration.\n+\n+    Basic Usage::\n+\n+      >>> import requests\n+      >>> s = requests.Session()\n+      >>> s.get('http://httpbin.org/get')\n+      200\n+    \"\"\"\n+\n+    def __init__(self):\n+\n+        #: A case-insensitive dictionary of headers to be sent on each\n+        #: :class:`Request <Request>` sent from this\n+        #: :class:`Session <Session>`.\n+        self.headers = default_headers()\n+\n+        #: Default Authentication tuple or object to attach to\n+        #: :class:`Request <Request>`.\n+        self.auth = None\n+\n+        #: Dictionary mapping protocol to the URL of the proxy (e.g.\n+        #: {'http': 'foo.bar:3128'}) to be used on each\n+        #: :class:`Request <Request>`.\n+        self.proxies = {}\n+\n+        #: Event-handling hooks.\n+        self.hooks = default_hooks()\n+\n+        #: Dictionary of querystring data to attach to each\n+        #: :class:`Request <Request>`. The dictionary values may be lists for\n+        #: representing multivalued query parameters.\n+        self.params = {}\n+\n+        #: Stream response content default.\n+        self.stream = False\n+\n+        #: SSL Verification default.\n+        self.verify = True\n+\n+        #: SSL certificate default.\n+        self.cert = None\n+\n+        #: Maximum number of redirects to follow.\n+        self.max_redirects = DEFAULT_REDIRECT_LIMIT\n+\n+        #: Should we trust the environment?\n+        self.trust_env = True\n+\n+        # Set up a CookieJar to be used by default\n+        self.cookies = cookiejar_from_dict({})\n+\n+        # Default connection adapters.\n+        self.adapters = {}\n+        self.mount('http://', HTTPAdapter())\n+        self.mount('https://', HTTPAdapter())\n+\n+    def __enter__(self):\n+        return self\n+\n+    def __exit__(self, *args):\n+        self.close()\n+\n+    def request(self, method, url,\n+        params=None,\n+        data=None,\n+        headers=None,\n+        cookies=None,\n+        files=None,\n+        auth=None,\n+        timeout=None,\n+        allow_redirects=True,\n+        proxies=None,\n+        hooks=None,\n+        stream=None,\n+        verify=None,\n+        cert=None):\n+\n+        cookies = cookies or {}\n+        proxies = proxies or {}\n+\n+        # Bootstrap CookieJar.\n+        if not isinstance(cookies, cookielib.CookieJar):\n+            cookies = cookiejar_from_dict(cookies)\n+\n+        # Bubble down session cookies.\n+        for cookie in self.cookies:\n+            cookies.set_cookie(cookie)\n+\n+        # Gather clues from the surrounding environment.\n+        if self.trust_env:\n+            # Set environment's proxies.\n+            env_proxies = get_environ_proxies(url) or {}\n+            for (k, v) in env_proxies.items():\n+                proxies.setdefault(k, v)\n+\n+            # Set environment's basic authentication.\n+            if not auth:\n+                auth = get_netrc_auth(url)\n+\n+            # Look for configuration.\n+            if not verify and verify is not False:\n+                verify = os.environ.get('REQUESTS_CA_BUNDLE')\n+\n+            # Curl compatibility.\n+            if not verify and verify is not False:\n+                verify = os.environ.get('CURL_CA_BUNDLE')\n+\n+\n+        # Merge all the kwargs.\n+        params = merge_kwargs(params, self.params)\n+        headers = merge_kwargs(headers, self.headers)\n+        auth = merge_kwargs(auth, self.auth)\n+        proxies = merge_kwargs(proxies, self.proxies)\n+        hooks = merge_kwargs(hooks, self.hooks)\n+        stream = merge_kwargs(stream, self.stream)\n+        verify = merge_kwargs(verify, self.verify)\n+        cert = merge_kwargs(cert, self.cert)\n+\n+\n+        # Create the Request.\n+        req = Request()\n+        req.method = method.upper()\n+        req.url = url\n+        req.headers = headers\n+        req.files = files\n+        req.data = data\n+        req.params = params\n+        req.auth = auth\n+        req.cookies = cookies\n+        req.hooks = hooks\n+\n+        # Prepare the Request.\n+        prep = req.prepare()\n+\n+        # Send the request.\n+        resp = self.send(prep, stream=stream, timeout=timeout, verify=verify, \ncert=cert, proxies=proxies)\n+\n+        # Persist cookies.\n+        for cookie in resp.cookies:\n+            self.cookies.set_cookie(cookie)\n+\n+        # Redirect resolving generator.\n+        gen = self.resolve_redirects(resp, req, stream=stream, timeout=timeout,\nverify=verify, cert=cert, proxies=proxies)\n+\n+        # Resolve redirects if allowed.\n+        history = [r for r in gen] if allow_redirects else []\n+\n+        # Shuffle things around if there's history.\n+        if history:\n+            history.insert(0, resp)\n+            resp = history.pop()\n+            resp.history = tuple(history)\n+\n+        return resp\n+\n+    def get(self, url, **kwargs):\n+        \"\"\"Sends a GET request. Returns :class:`Response` object.\n+\n+        :param url: URL for the new :class:`Request` object.\n+        :param \\*\\*kwargs: Optional arguments that ``request`` takes.\n+        \"\"\"\n+\n+        kwargs.setdefault('allow_redirects', True)\n+        return self.request('GET', url, **kwargs)\n+\n+    def options(self, url, **kwargs):\n+        \"\"\"Sends a OPTIONS request. Returns :class:`Response` object.\n+\n+        :param url: URL for the new :class:`Request` object.\n+        :param \\*\\*kwargs: Optional arguments that ``request`` takes.\n+        \"\"\"\n+\n+        kwargs.setdefault('allow_redirects', True)\n+        return self.request('OPTIONS', url, **kwargs)\n+\n+    def head(self, url, **kwargs):\n+        \"\"\"Sends a HEAD request. Returns :class:`Response` object.\n+\n+        :param url: URL for the new :class:`Request` object.\n+        :param \\*\\*kwargs: Optional arguments that ``request`` takes.\n+        \"\"\"\n+\n+        kwargs.setdefault('allow_redirects', False)\n+        return self.request('HEAD', url, **kwargs)\n+\n+    def post(self, url, data=None, **kwargs):\n+        \"\"\"Sends a POST request. Returns :class:`Response` object.\n+\n+        :param url: URL for the new :class:`Request` object.\n+        :param data: (optional) Dictionary, bytes, or file-like object to send \nin the body of the :class:`Request`.\n+        :param \\*\\*kwargs: Optional arguments that ``request`` takes.\n+        \"\"\"\n+\n+        return self.request('POST', url, data=data, **kwargs)\n+\n+    def put(self, url, data=None, **kwargs):\n+        \"\"\"Sends a PUT request. Returns :class:`Response` object.\n+\n+        :param url: URL for the new :class:`Request` object.\n+        :param data: (optional) Dictionary, bytes, or file-like object to send \nin the body of the :class:`Request`.\n+        :param \\*\\*kwargs: Optional arguments that ``request`` takes.\n+        \"\"\"\n+\n+        return self.request('PUT', url, data=data, **kwargs)\n+\n+    def patch(self, url, data=None, **kwargs):\n+        \"\"\"Sends a PATCH request. Returns :class:`Response` object.\n+\n+        :param url: URL for the new :class:`Request` object.\n+        :param data: (optional) Dictionary, bytes, or file-like object to send \nin the body of the :class:`Request`.\n+        :param \\*\\*kwargs: Optional arguments that ``request`` takes.\n+        \"\"\"\n+\n+        return self.request('PATCH', url,  data=data, **kwargs)\n+\n+    def delete(self, url, **kwargs):\n+        \"\"\"Sends a DELETE request. Returns :class:`Response` object.\n+\n+        :param url: URL for the new :class:`Request` object.\n+        :param \\*\\*kwargs: Optional arguments that ``request`` takes.\n+        \"\"\"\n+\n+        return self.request('DELETE', url, **kwargs)\n+\n+    def send(self, request, **kwargs):\n+        \"\"\"Send a given PreparedRequest.\"\"\"\n+        hooks = request.hooks\n+        adapter = self.get_adapter(url=request.url)\n+        r = adapter.send(request, **kwargs)\n+        # Response manipulation hooks\n+        r = dispatch_hook('response', hooks, r)\n+        return r\n+\n+    def get_adapter(self, url):\n+        \"\"\"Returns the appropriate connnection adapter for the given URL.\"\"\"\n+        for (prefix, adapter) in self.adapters.items():\n+\n+            if url.startswith(prefix):\n+                return adapter\n+\n+        # Nothing matches :-/\n+        raise InvalidSchema(\"No connection adapters were found for '%s'\" % url)\n+\n+    def close(self):\n+        \"\"\"Closes all adapters and as such the session\"\"\"\n+        for _, v in self.adapters.items():\n+            v.close()\n+\n+    def mount(self, prefix, adapter):\n+        \"\"\"Registers a connection adapter to a prefix.\"\"\"\n+        self.adapters[prefix] = adapter\n+\n+    def __getstate__(self):\n+        return dict((attr, getattr(self, attr, None)) for attr in \nself.__attrs__)\n+\n+    def __setstate__(self, state):\n+        for attr, value in state.items():\n+            setattr(self, attr, value)\n+\n+\n+def session():\n+    \"\"\"Returns a :class:`Session` for context-management.\"\"\"\n+\n+    return Session()\ndiff --git a/build/lib/requests/status_codes.py \nb/build/lib/requests/status_codes.py\nnew file mode 100644\nindex 0000000..08edab4\n--- /dev/null\n+++ b/build/lib/requests/status_codes.py\n@@ -0,0 +1,86 @@\n+# -*- coding: utf-8 -*-\n+\n+from .structures import LookupDict\n+\n+_codes = {\n+\n+    # Informational.\n+    100: ('continue',),\n+    101: ('switching_protocols',),\n+    102: ('processing',),\n+    103: ('checkpoint',),\n+    122: ('uri_too_long', 'request_uri_too_long'),\n+    200: ('ok', 'okay', 'all_ok', 'all_okay', 'all_good', '\\\\o/', '\u2713'),\n+    201: ('created',),\n+    202: ('accepted',),\n+    203: ('non_authoritative_info', 'non_authoritative_information'),\n+    204: ('no_content',),\n+    205: ('reset_content', 'reset'),\n+    206: ('partial_content', 'partial'),\n+    207: ('multi_status', 'multiple_status', 'multi_stati', 'multiple_stati'),\n+    208: ('im_used',),\n+\n+    # Redirection.\n+    300: ('multiple_choices',),\n+    301: ('moved_permanently', 'moved', '\\\\o-'),\n+    302: ('found',),\n+    303: ('see_other', 'other'),\n+    304: ('not_modified',),\n+    305: ('use_proxy',),\n+    306: ('switch_proxy',),\n+    307: ('temporary_redirect', 'temporary_moved', 'temporary'),\n+    308: ('resume_incomplete', 'resume'),\n+\n+    # Client Error.\n+    400: ('bad_request', 'bad'),\n+    401: ('unauthorized',),\n+    402: ('payment_required', 'payment'),\n+    403: ('forbidden',),\n+    404: ('not_found', '-o-'),\n+    405: ('method_not_allowed', 'not_allowed'),\n+    406: ('not_acceptable',),\n+    407: ('proxy_authentication_required', 'proxy_auth', \n'proxy_authentication'),\n+    408: ('request_timeout', 'timeout'),\n+    409: ('conflict',),\n+    410: ('gone',),\n+    411: ('length_required',),\n+    412: ('precondition_failed', 'precondition'),\n+    413: ('request_entity_too_large',),\n+    414: ('request_uri_too_large',),\n+    415: ('unsupported_media_type', 'unsupported_media', 'media_type'),\n+    416: ('requested_range_not_satisfiable', 'requested_range', \n'range_not_satisfiable'),\n+    417: ('expectation_failed',),\n+    418: ('im_a_teapot', 'teapot', 'i_am_a_teapot'),\n+    422: ('unprocessable_entity', 'unprocessable'),\n+    423: ('locked',),\n+    424: ('failed_dependency', 'dependency'),\n+    425: ('unordered_collection', 'unordered'),\n+    426: ('upgrade_required', 'upgrade'),\n+    428: ('precondition_required', 'precondition'),\n+    429: ('too_many_requests', 'too_many'),\n+    431: ('header_fields_too_large', 'fields_too_large'),\n+    444: ('no_response', 'none'),\n+    449: ('retry_with', 'retry'),\n+    450: ('blocked_by_windows_parental_controls', 'parental_controls'),\n+    499: ('client_closed_request',),\n+\n+    # Server Error.\n+    500: ('internal_server_error', 'server_error', '/o\\\\', '\u2717'),\n+    501: ('not_implemented',),\n+    502: ('bad_gateway',),\n+    503: ('service_unavailable', 'unavailable'),\n+    504: ('gateway_timeout',),\n+    505: ('http_version_not_supported', 'http_version'),\n+    506: ('variant_also_negotiates',),\n+    507: ('insufficient_storage',),\n+    509: ('bandwidth_limit_exceeded', 'bandwidth'),\n+    510: ('not_extended',),\n+}\n+\n+codes = LookupDict(name='status_codes')\n+\n+for (code, titles) in list(_codes.items()):\n+    for title in titles:\n+        setattr(codes, title, code)\n+        if not title.startswith('\\\\'):\n+            setattr(codes, title.upper(), code)\ndiff --git a/build/lib/requests/structures.py b/build/lib/requests/structures.py\nnew file mode 100644\nindex 0000000..6c2e0b2\n--- /dev/null\n+++ b/build/lib/requests/structures.py\n@@ -0,0 +1,89 @@\n+# -*- coding: utf-8 -*-\n+\n+\"\"\"\n+requests.structures\n+~~~~~~~~~~~~~~~~~~~\n+\n+Data structures that power Requests.\n+\n+\"\"\"\n+\n+import os\n+from itertools import islice\n+\n+class IteratorProxy(object):\n+    \"\"\"docstring for IteratorProxy\"\"\"\n+    def __init__(self, i):\n+        self.i = i\n+        # self.i = chain.from_iterable(i)\n+\n+    def __iter__(self):\n+        return self.i\n+\n+    def __len__(self):\n+        if hasattr(self.i, '__len__'):\n+            return len(self.i)\n+        if hasattr(self.i, 'len'):\n+            return self.i.len\n+        if hasattr(self.i, 'fileno'):\n+            return os.fstat(self.i.fileno()).st_size\n+\n+    def read(self, n):\n+        return \"\".join(islice(self.i, None, n))\n+\n+class CaseInsensitiveDict(dict):\n+    \"\"\"Case-insensitive Dictionary\n+\n+    For example, ``headers['content-encoding']`` will return the\n+    value of a ``'Content-Encoding'`` response header.\"\"\"\n+\n+    @property\n+    def lower_keys(self):\n+        if not hasattr(self, '_lower_keys') or not self._lower_keys:\n+            self._lower_keys = dict((k.lower(), k) for k in list(self.keys()))\n+        return self._lower_keys\n+\n+    def _clear_lower_keys(self):\n+        if hasattr(self, '_lower_keys'):\n+            self._lower_keys.clear()\n+\n+    def __setitem__(self, key, value):\n+        dict.__setitem__(self, key, value)\n+        self._clear_lower_keys()\n+\n+    def __delitem__(self, key):\n+        dict.__delitem__(self, self.lower_keys.get(key.lower(), key))\n+        self._lower_keys.clear()\n+\n+    def __contains__(self, key):\n+        return key.lower() in self.lower_keys\n+\n+    def __getitem__(self, key):\n+        # We allow fall-through here, so values default to None\n+        if key in self:\n+            return dict.__getitem__(self, self.lower_keys[key.lower()])\n+\n+    def get(self, key, default=None):\n+        if key in self:\n+            return self[key]\n+        else:\n+            return default\n+\n+\n+class LookupDict(dict):\n+    \"\"\"Dictionary lookup object.\"\"\"\n+\n+    def __init__(self, name=None):\n+        self.name = name\n+        super(LookupDict, self).__init__()\n+\n+    def __repr__(self):\n+        return '<lookup \\'%s\\'>' % (self.name)\n+\n+    def __getitem__(self, key):\n+        # We allow fall-through here, so values default to None\n+\n+        return self.__dict__.get(key, None)\n+\n+    def get(self, key, default=None):\n+        return self.__dict__.get(key, default)\ndiff --git a/build/lib/requests/utils.py b/build/lib/requests/utils.py\nnew file mode 100644\nindex 0000000..34d92d2\n--- /dev/null\n+++ b/build/lib/requests/utils.py\n@@ -0,0 +1,582 @@\n+# -*- coding: utf-8 -*-\n+\n+\"\"\"\n+requests.utils\n+~~~~~~~~~~~~~~\n+\n+This module provides utility functions that are used within Requests\n+that are also useful for external consumption.\n+\n+\"\"\"\n+\n+import cgi\n+import codecs\n+import os\n+import platform\n+import re\n+import sys\n+import zlib\n+from netrc import netrc, NetrcParseError\n+\n+from . import __version__\n+from . import certs\n+from .compat import parse_http_list as _parse_list_header\n+from .compat import quote, urlparse, bytes, str, OrderedDict, urlunparse\n+from .cookies import RequestsCookieJar, cookiejar_from_dict\n+\n+_hush_pyflakes = (RequestsCookieJar,)\n+\n+NETRC_FILES = ('.netrc', '_netrc')\n+\n+DEFAULT_CA_BUNDLE_PATH = certs.where()\n+\n+def dict_to_sequence(d):\n+    \"\"\"Returns an internal sequence dictionary update.\"\"\"\n+\n+    if hasattr(d, 'items'):\n+        d = d.items()\n+\n+    return d\n+\n+def super_len(o):\n+    if hasattr(o, '__len__'):\n+        return len(o)\n+    if hasattr(o, 'len'):\n+        return o.len\n+    if hasattr(o, 'fileno'):\n+        return os.fstat(o.fileno()).st_size\n+\n+def get_netrc_auth(url):\n+    \"\"\"Returns the Requests tuple auth for a given url from netrc.\"\"\"\n+\n+    try:\n+        locations = (os.path.expanduser('~/{0}'.format(f)) for f in \nNETRC_FILES)\n+        netrc_path = None\n+\n+        for loc in locations:\n+            if os.path.exists(loc) and not netrc_path:\n+                netrc_path = loc\n+\n+        # Abort early if there isn't one.\n+        if netrc_path is None:\n+            return netrc_path\n+\n+        ri = urlparse(url)\n+\n+        # Strip port numbers from netloc\n+        host = ri.netloc.split(':')[0]\n+\n+        try:\n+            _netrc = netrc(netrc_path).authenticators(host)\n+            if _netrc:\n+                # Return with login / password\n+                login_i = (0 if _netrc[0] else 1)\n+                return (_netrc[login_i], _netrc[2])\n+        except (NetrcParseError, IOError):\n+            # If there was a parsing error or a permissions issue reading the \nfile,\n+            # we'll just skip netrc auth\n+            pass\n+\n+    # AppEngine hackiness.\n+    except (ImportError, AttributeError):\n+        pass\n+\n+\n+def guess_filename(obj):\n+    \"\"\"Tries to guess the filename of the given object.\"\"\"\n+    name = getattr(obj, 'name', None)\n+    if name and name[0] != '<' and name[-1] != '>':\n+        return name\n+\n+\n+def from_key_val_list(value):\n+    \"\"\"Take an object and test to see if it can be represented as a\n+    dictionary. Unless it can not be represented as such, return an\n+    OrderedDict, e.g.,\n+\n+    ::\n+\n+        >>> from_key_val_list([('key', 'val')])\n+        OrderedDict([('key', 'val')])\n+        >>> from_key_val_list('string')\n+        ValueError: need more than 1 value to unpack\n+        >>> from_key_val_list({'key': 'val'})\n+        OrderedDict([('key', 'val')])\n+    \"\"\"\n+    if value is None:\n+        return None\n+\n+    if isinstance(value, (str, bytes, bool, int)):\n+        raise ValueError('cannot encode objects that are not 2-tuples')\n+\n+    return OrderedDict(value)\n+\n+\n+def to_key_val_list(value):\n+    \"\"\"Take an object and test to see if it can be represented as a\n+    dictionary. If it can be, return a list of tuples, e.g.,\n+\n+    ::\n+\n+        >>> to_key_val_list([('key', 'val')])\n+        [('key', 'val')]\n+        >>> to_key_val_list({'key': 'val'})\n+        [('key', 'val')]\n+        >>> to_key_val_list('string')\n+        ValueError: cannot encode objects that are not 2-tuples.\n+    \"\"\"\n+    if value is None:\n+        return None\n+\n+    if isinstance(value, (str, bytes, bool, int)):\n+        raise ValueError('cannot encode objects that are not 2-tuples')\n+\n+    if isinstance(value, dict):\n+        value = value.items()\n+\n+    return list(value)\n+\n+\n+# From mitsuhiko/werkzeug (used with permission).\n+def parse_list_header(value):\n+    \"\"\"Parse lists as described by RFC 2068 Section 2.\n+\n+    In particular, parse comma-separated lists where the elements of\n+    the list may include quoted-strings.  A quoted-string could\n+    contain a comma.  A non-quoted string could have quotes in the\n+    middle.  Quotes are removed automatically after parsing.\n+\n+    It basically works like :func:`parse_set_header` just that items\n+    may appear multiple times and case sensitivity is preserved.\n+\n+    The return value is a standard :class:`list`:\n+\n+    >>> parse_list_header('token, \"quoted value\"')\n+    ['token', 'quoted value']\n+\n+    To create a header from the :class:`list` again, use the\n+    :func:`dump_header` function.\n+\n+    :param value: a string with a list header.\n+    :return: :class:`list`\n+    \"\"\"\n+    result = []\n+    for item in _parse_list_header(value):\n+        if item[:1] == item[-1:] == '\"':\n+            item = unquote_header_value(item[1:-1])\n+        result.append(item)\n+    return result\n+\n+\n+# From mitsuhiko/werkzeug (used with permission).\n+def parse_dict_header(value):\n+    \"\"\"Parse lists of key, value pairs as described by RFC 2068 Section 2 and\n+    convert them into a python dict:\n+\n+    >>> d = parse_dict_header('foo=\"is a fish\", bar=\"as well\"')\n+    >>> type(d) is dict\n+    True\n+    >>> sorted(d.items())\n+    [('bar', 'as well'), ('foo', 'is a fish')]\n+\n+    If there is no value for a key it will be `None`:\n+\n+    >>> parse_dict_header('key_without_value')\n+    {'key_without_value': None}\n+\n+    To create a header from the :class:`dict` again, use the\n+    :func:`dump_header` function.\n+\n+    :param value: a string with a dict header.\n+    :return: :class:`dict`\n+    \"\"\"\n+    result = {}\n+    for item in _parse_list_header(value):\n+        if '=' not in item:\n+            result[item] = None\n+            continue\n+        name, value = item.split('=', 1)\n+        if value[:1] == value[-1:] == '\"':\n+            value = unquote_header_value(value[1:-1])\n+        result[name] = value\n+    return result\n+\n+\n+# From mitsuhiko/werkzeug (used with permission).\n+def unquote_header_value(value, is_filename=False):\n+    r\"\"\"Unquotes a header value.  (Reversal of :func:`quote_header_value`).\n+    This does not use the real unquoting but what browsers are actually\n+    using for quoting.\n+\n+    :param value: the header value to unquote.\n+    \"\"\"\n+    if value and value[0] == value[-1] == '\"':\n+        # this is not the real unquoting, but fixing this so that the\n+        # RFC is met will result in bugs with internet explorer and\n+        # probably some other browsers as well.  IE for example is\n+        # uploading files with \"C:\\foo\\bar.txt\" as filename\n+        value = value[1:-1]\n+\n+        # if this is a filename and the starting characters look like\n+        # a UNC path, then just return the value without quotes.  Using the\n+        # replace sequence below on a UNC path has the effect of turning\n+        # the leading double slash into a single slash and then\n+        # _fix_ie_filename() doesn't work correctly.  See #458.\n+        if not is_filename or value[:2] != '\\\\\\\\':\n+            return value.replace('\\\\\\\\', '\\\\').replace('\\\\\"', '\"')\n+    return value\n+\n+\n+def dict_from_cookiejar(cj):\n+    \"\"\"Returns a key/value dictionary from a CookieJar.\n+\n+    :param cj: CookieJar object to extract cookies from.\n+    \"\"\"\n+\n+    cookie_dict = {}\n+\n+    for cookie in cj:\n+        cookie_dict[cookie.name] = cookie.value\n+\n+    return cookie_dict\n+\n+\n+def add_dict_to_cookiejar(cj, cookie_dict):\n+    \"\"\"Returns a CookieJar from a key/value dictionary.\n+\n+    :param cj: CookieJar to insert cookies into.\n+    :param cookie_dict: Dict of key/values to insert into CookieJar.\n+    \"\"\"\n+\n+    cj2 = cookiejar_from_dict(cookie_dict)\n+    for cookie in cj2:\n+        cj.set_cookie(cookie)\n+    return cj\n+\n+\n+def get_encodings_from_content(content):\n+    \"\"\"Returns encodings from given content string.\n+\n+    :param content: bytestring to extract encodings from.\n+    \"\"\"\n+\n+    charset_re = re.compile(r'<meta.*?charset=[\"\\']*(.+?)[\"\\'>]', flags=re.I)\n+\n+    return charset_re.findall(content)\n+\n+\n+def get_encoding_from_headers(headers):\n+    \"\"\"Returns encodings from given HTTP Header Dict.\n+\n+    :param headers: dictionary to extract encoding from.\n+    \"\"\"\n+\n+    content_type = headers.get('content-type')\n+\n+    if not content_type:\n+        return None\n+\n+    content_type, params = cgi.parse_header(content_type)\n+\n+    if 'charset' in params:\n+        return params['charset'].strip(\"'\\\"\")\n+\n+    if 'text' in content_type:\n+        return 'ISO-8859-1'\n+\n+\n+def stream_decode_response_unicode(iterator, r):\n+    \"\"\"Stream decodes a iterator.\"\"\"\n+\n+    if r.encoding is None:\n+        for item in iterator:\n+            yield item\n+        return\n+\n+    decoder = codecs.getincrementaldecoder(r.encoding)(errors='replace')\n+    for chunk in iterator:\n+        rv = decoder.decode(chunk)\n+        if rv:\n+            yield rv\n+    rv = decoder.decode('', final=True)\n+    if rv:\n+        yield rv\n+\n+\n+def iter_slices(string, slice_length):\n+    \"\"\"Iterate over slices of a string.\"\"\"\n+    pos = 0\n+    while pos < len(string):\n+        yield string[pos:pos + slice_length]\n+        pos += slice_length\n+\n+\n+def get_unicode_from_response(r):\n+    \"\"\"Returns the requested content back in unicode.\n+\n+    :param r: Response object to get unicode content from.\n+\n+    Tried:\n+\n+    1. charset from content-type\n+\n+    2. every encodings from ``<meta ... charset=XXX>``\n+\n+    3. fall back and replace all unicode characters\n+\n+    \"\"\"\n+\n+    tried_encodings = []\n+\n+    # Try charset from content-type\n+    encoding = get_encoding_from_headers(r.headers)\n+\n+    if encoding:\n+        try:\n+            return str(r.content, encoding)\n+        except UnicodeError:\n+            tried_encodings.append(encoding)\n+\n+    # Fall back:\n+    try:\n+        return str(r.content, encoding, errors='replace')\n+    except TypeError:\n+        return r.content\n+\n+\n+def stream_decompress(iterator, mode='gzip'):\n+    \"\"\"Stream decodes an iterator over compressed data\n+\n+    :param iterator: An iterator over compressed data\n+    :param mode: 'gzip' or 'deflate'\n+    :return: An iterator over decompressed data\n+    \"\"\"\n+\n+    if mode not in ['gzip', 'deflate']:\n+        raise ValueError('stream_decompress mode must be gzip or deflate')\n+\n+    zlib_mode = 16 + zlib.MAX_WBITS if mode == 'gzip' else -zlib.MAX_WBITS\n+    dec = zlib.decompressobj(zlib_mode)\n+    try:\n+        for chunk in iterator:\n+            rv = dec.decompress(chunk)\n+            if rv:\n+                yield rv\n+    except zlib.error:\n+        # If there was an error decompressing, just return the raw chunk\n+        yield chunk\n+        # Continue to return the rest of the raw data\n+        for chunk in iterator:\n+            yield chunk\n+    else:\n+        # Make sure everything has been returned from the decompression object\n+        buf = dec.decompress(bytes())\n+        rv = buf + dec.flush()\n+        if rv:\n+            yield rv\n+\n+\n+def stream_untransfer(gen, resp):\n+    ce = resp.headers.get('content-encoding', '').lower()\n+    if 'gzip' in ce:\n+        gen = stream_decompress(gen, mode='gzip')\n+    elif 'deflate' in ce:\n+        gen = stream_decompress(gen, mode='deflate')\n+\n+    return gen\n+\n+\n+# The unreserved URI characters (RFC 3986)\n+UNRESERVED_SET = frozenset(\n+    \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\"\n+    + \"0123456789-._~\")\n+\n+\n+def unquote_unreserved(uri):\n+    \"\"\"Un-escape any percent-escape sequences in a URI that are unreserved\n+    characters. This leaves all reserved, illegal and non-ASCII bytes encoded.\n+    \"\"\"\n+    parts = uri.split('%')\n+    for i in range(1, len(parts)):\n+        h = parts[i][0:2]\n+        if len(h) == 2 and h.isalnum():\n+            c = chr(int(h, 16))\n+            if c in UNRESERVED_SET:\n+                parts[i] = c + parts[i][2:]\n+            else:\n+                parts[i] = '%' + parts[i]\n+        else:\n+            parts[i] = '%' + parts[i]\n+    return ''.join(parts)\n+\n+\n+def requote_uri(uri):\n+    \"\"\"Re-quote the given URI.\n+\n+    This function passes the given URI through an unquote/quote cycle to\n+    ensure that it is fully and consistently quoted.\n+    \"\"\"\n+    # Unquote only the unreserved characters\n+    # Then quote only illegal characters (do not quote reserved, unreserved,\n+    # or '%')\n+    return quote(unquote_unreserved(uri), safe=\"!#$%&'()*+,/:;=?@[]~\")\n+\n+\n+def get_environ_proxies(url):\n+    \"\"\"Return a dict of environment proxies.\"\"\"\n+\n+    proxy_keys = [\n+        'all',\n+        'http',\n+        'https',\n+        'ftp',\n+        'socks'\n+    ]\n+\n+    get_proxy = lambda k: os.environ.get(k) or os.environ.get(k.upper())\n+\n+    # First check whether no_proxy is defined. If it is, check that the URL\n+    # we're getting isn't in the no_proxy list.\n+    no_proxy = get_proxy('no_proxy')\n+\n+    if no_proxy:\n+        # We need to check whether we match here. We need to see if we match\n+        # the end of the netloc, both with and without the port.\n+        no_proxy = no_proxy.split(',')\n+        netloc = urlparse(url).netloc\n+\n+        for host in no_proxy:\n+            if netloc.endswith(host) or netloc.split(':')[0].endswith(host):\n+                # The URL does match something in no_proxy, so we don't want\n+                # to apply the proxies on this URL.\n+                return {}\n+\n+    # If we get here, we either didn't have no_proxy set or we're not going\n+    # anywhere that no_proxy applies to.\n+    proxies = [(key, get_proxy(key + '_proxy')) for key in proxy_keys]\n+    return dict([(key, val) for (key, val) in proxies if val])\n+\n+\n+def default_user_agent():\n+    \"\"\"Return a string representing the default user agent.\"\"\"\n+    _implementation = platform.python_implementation()\n+\n+    if _implementation == 'CPython':\n+        _implementation_version = platform.python_version()\n+    elif _implementation == 'PyPy':\n+        _implementation_version = '%s.%s.%s' % (\n+                                                sys.pypy_version_info.major,\n+                                                sys.pypy_version_info.minor,\n+                                                sys.pypy_version_info.micro\n+                                            )\n+        if sys.pypy_version_info.releaselevel != 'final':\n+            _implementation_version = ''.join([_implementation_version, \nsys.pypy_version_info.releaselevel])\n+    elif _implementation == 'Jython':\n+        _implementation_version = platform.python_version()  # Complete Guess\n+    elif _implementation == 'IronPython':\n+        _implementation_version = platform.python_version()  # Complete Guess\n+    else:\n+        _implementation_version = 'Unknown'\n+\n+    try:\n+        p_system = platform.system()\n+        p_release = platform.release()\n+    except IOError:\n+        p_system = 'Unknown'\n+        p_release = 'Unknown'\n+\n+    return \" \".join([\n+            'python-requests/%s' % __version__,\n+            '%s/%s' % (_implementation, _implementation_version),\n+            '%s/%s' % (p_system, p_release),\n+        ])\n+\n+def default_headers():\n+    return {\n+        'User-Agent': default_user_agent(),\n+        'Accept-Encoding': ', '.join(('gzip', 'deflate', 'compress')),\n+        'Accept': '*/*'\n+    }\n+\n+\n+def parse_header_links(value):\n+    \"\"\"Return a dict of parsed link headers proxies.\n+\n+    i.e. Link: <http:/.../front.jpeg>; rel=front; \ntype=\"image/jpeg\",<http://.../back.jpeg>; rel=back;type=\"image/jpeg\"\n+\n+    \"\"\"\n+\n+    links = []\n+\n+    replace_chars = \" '\\\"\"\n+\n+    for val in value.split(\",\"):\n+        try:\n+            url, params = val.split(\";\", 1)\n+        except ValueError:\n+            url, params = val, ''\n+\n+        link = {}\n+\n+        link[\"url\"] = url.strip(\"<> '\\\"\")\n+\n+        for param in params.split(\";\"):\n+            try:\n+                key,value = param.split(\"=\")\n+            except ValueError:\n+                break\n+\n+            link[key.strip(replace_chars)] = value.strip(replace_chars)\n+\n+        links.append(link)\n+\n+    return links\n+\n+\n+# Null bytes; no need to recreate these on each call to guess_json_utf\n+_null = '\\x00'.encode('ascii')  # encoding to ASCII for Python 3\n+_null2 = _null * 2\n+_null3 = _null * 3\n+\n+\n+def guess_json_utf(data):\n+    # JSON always starts with two ASCII characters, so detection is as\n+    # easy as counting the nulls and from their location and count\n+    # determine the encoding. Also detect a BOM, if present.\n+    sample = data[:4]\n+    if sample in (codecs.BOM_UTF32_LE, codecs.BOM32_BE):\n+        return 'utf-32'     # BOM included\n+    if sample[:3] == codecs.BOM_UTF8:\n+        return 'utf-8-sig'  # BOM included, MS style (discouraged)\n+    if sample[:2] in (codecs.BOM_UTF16_LE, codecs.BOM_UTF16_BE):\n+        return 'utf-16'     # BOM included\n+    nullcount = sample.count(_null)\n+    if nullcount == 0:\n+        return 'utf-8'\n+    if nullcount == 2:\n+        if sample[::2] == _null2:   # 1st and 3rd are null\n+            return 'utf-16-be'\n+        if sample[1::2] == _null2:  # 2nd and 4th are null\n+            return 'utf-16-le'\n+        # Did not detect 2 valid UTF-16 ascii-range characters\n+    if nullcount == 3:\n+        if sample[:3] == _null3:\n+            return 'utf-32-be'\n+        if sample[1:] == _null3:\n+            return 'utf-32-le'\n+        # Did not detect a valid UTF-32 ascii-range character\n+    return None\n+\n+\n+def prepend_scheme_if_needed(url, new_scheme):\n+    '''Given a URL that may or may not have a scheme, prepend the given scheme.\n+    Does not replace a present scheme with the one provided as an argument.'''\n+    scheme, netloc, path, params, query, fragment = urlparse(url, new_scheme)\n+\n+    # urlparse is a finicky beast, and sometimes decides that there isn't a\n+    # netloc present. Assume that it's being over-cautious, and switch netloc\n+    # and path if urlparse decided there was no netloc.\n+    if not netloc:\n+        netloc, path = path, netloc\n+\n+    return urlunparse((scheme, netloc, path, params, query, fragment))\ndiff --git a/requests/models.py b/requests/models.py\nindex 9926045..cbd693f 100644\n--- a/requests/models.py\n+++ b/requests/models.py\n@@ -386,12 +386,15 @@ class PreparedRequest(RequestEncodingMixin, \nRequestHooksMixin):\n         self.body = body\n \n     def prepare_content_length(self, body):\n-        self.headers['Content-Length'] = '0'\n+        # Only set Content-Length if there is a body. Some servers (e.g. \nAmazon) reject\n+        # GET requests that include a Content-Length header with no body.\n+        if body is None:\n+            return\n         if hasattr(body, 'seek') and hasattr(body, 'tell'):\n             body.seek(0, 2)\n             self.headers['Content-Length'] = str(body.tell())\n             body.seek(0, 0)\n-        elif body is not None:\n+        else:\n             self.headers['Content-Length'] = str(len(body))\n \n     def prepare_auth(self, auth):\n\nSaved trajectory to '/results/phase_a_v0_verified_280_300_20260129_171549/psf__requests-1142/psf__requests-1142.traj.json'\n\ud83e\udd96 DEBUG    Ensuring deployment is stopped because object is deleted            \n",
  "stderr": "Warning: Input is not a terminal (fd=0).\n/usr/local/lib/python3.12/site-packages/click/core.py:1223: UserWarning: The parameter -c is used more than once. Remove its duplicate as parameters should be unique.\n  parser = self.make_parser(ctx)\n/usr/local/lib/python3.12/site-packages/click/core.py:1216: UserWarning: The parameter -c is used more than once. Remove its duplicate as parameters should be unique.\n  self.parse_args(ctx, args)\n",
  "attempts": 2,
  "end_time": "2026-01-29T09:55:37.573034",
  "duration_seconds": 390.686787
}